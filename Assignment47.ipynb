{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "# Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?\n\nRidge Regression is a regularized form of linear regression that adds a penalty (L2 regularization) to the cost function. The model minimizes the sum of squared errors along with the sum of squared coefficients, which shrinks the coefficients toward zero but does not eliminate them. Unlike ordinary least squares (OLS) regression, which only minimizes the error without any penalty, Ridge helps prevent overfitting, especially in the presence of multicollinearity.\n\n# Q2. What are the assumptions of Ridge Regression?\n\nThe relationship between the independent and dependent variables is linear.\nThere is no strong multicollinearity among the predictors.\nThe residuals are normally distributed (though Ridge can still perform well even with mild deviations).\n\n# Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression\n\nThe value of λ is typically selected using cross-validation. The objective is to find the \nλ that minimizes the model's generalization error on unseen data.\n\n# Q4. Can Ridge Regression be used for feature selection? If yes, how\n\nRidge Regression does not perform feature selection because it shrinks the coefficients but does not set them to zero. It reduces the influence of less important features \nbut keeps all features in the model.\n\n# Q5. How does the Ridge Regression model perform in the presence of multicollinearity?\n\nRidge Regression performs well in the presence of multicollinearity by shrinking the coefficients of correlated predictors, thus reducing their impact and improving model stability.\n\n# Q6. Can Ridge Regression handle both categorical and continuous independent variables?\n\nYes, Ridge Regression can handle both categorical (after encoding, like one-hot encoding) and continuous independent variables.\n\n# Q7. How do you interpret the coefficients of Ridge Regression?\n\nThe coefficients in Ridge Regression represent the relationship between each feature and the target variable, but they are regularized (shrunk) versions of the OLS coefficients. \nLarger values of λ lead to smaller coefficients, indicating less influence of the corresponding features.\n\n# Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?\n\nYes, Ridge Regression can be used for time-series data, especially in forecasting models where the predictors include lagged values. It helps in managing collinearity between \npredictors (like lagged values) and prevents overfitting in high-dimensional time-series data.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}